{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python Functions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LPPQ_dwuMB"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-VgaWBcuTl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ea9d81b-7baf-49be-9dac-6864a804c983"
      },
      "source": [
        "!pip install num2words\n",
        "!pip install vaderSentiment\n",
        "!pip install streamlit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n",
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting streamlit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/0c/469ee9160ad7bc064eb498fa95aefd4e96b593ce0d53fb07ff217badff47/streamlit-0.83.0-py2.py3-none-any.whl (7.7MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Collecting blinker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting watchdog; platform_system != \"Darwin\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/5b/36b3b11e557830de6fc1dc06e9aa3ee274119b8cea9cc98175dbbf72cf87/watchdog-2.1.2-py3-none-manylinux2014_x86_64.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 36.0MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (57.0.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/6d/6c8fe4b658f77947d4244ce81f60230c4c8d1dc1a21ae83e63b269339178/ipykernel-5.5.5-py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal->streamlit) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13476 sha256=c5f88c8f711106c1c00ba6993ed5bd64c0157f200b838b1c65c7ca47a63dc8f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
            "Successfully built blinker\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blinker, watchdog, smmap, gitdb, gitpython, validators, ipykernel, pydeck, base58, streamlit\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.14 ipykernel-5.5.5 pydeck-0.6.2 smmap-4.0.0 streamlit-0.83.0 validators-0.18.2 watchdog-2.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipykernel"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iniTypUsv35c"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UJ9LVf8u1Qh",
        "outputId": "7c07a6ff-75b0-4c9a-ed87-6fe3cdb7b70b"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import gensim\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import nltk\n",
        "import string\n",
        "from num2words import num2words\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "SCRIPT_PATH = '/content/content/MyDrive/Star Wars Side Project/Scripts'\n",
        "\n",
        "import gensim.downloader as api\n",
        "model = api.load('glove-wiki-gigaword-50') #downloading a large pre-trained model\n",
        "\n",
        "sw = set()\n",
        "for word in STOPWORDS: #removing punctuation since we'll be filtering after removing punctuation\n",
        "  sw.add(word.translate(str.maketrans('', '', string.punctuation)))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPP0pvJnV5aB"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLlWYjx5V_BG"
      },
      "source": [
        "\"\"\"\n",
        "Read in script and return dataframe with character and line for each line in the movie\n",
        "\"\"\"\n",
        "\n",
        "def read_script(script_name):\n",
        "  with open(os.path.join(SCRIPT_PATH, script_name), 'r') as file: #SCRIPT_PATH defined above\n",
        "    script_full = file.read()\n",
        "\n",
        "  script_full = re.sub(r'\\\"[0-9]+\\\"', '', script_full) #reformatting script based on Star Wars styling\n",
        "  script_full = script_full.replace('\"character\" \"dialogue\"\\n ', '')\n",
        "\n",
        "  script = re.findall(r'\\\"[A-Z]+\\\"\\s\\\".*\\\"\\n', script_full)\n",
        "  clean_script = []\n",
        "  for line in script:\n",
        "    character, dialogue = line.split('\" \"')\n",
        "    character = character.replace('\"', '')\n",
        "    dialogue = dialogue.replace('\"', '')\n",
        "    dialogue = dialogue.replace('\\n', '')\n",
        "    clean_script.append([character, dialogue])\n",
        "\n",
        "  script_df = pd.DataFrame(clean_script, columns=['character', 'line']) #dataframe with character and line for each line in the movie\n",
        "  return script_df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbb56zaFWAiM"
      },
      "source": [
        "\"\"\"\n",
        "Preprocessing script text\n",
        "\"\"\"\n",
        "\n",
        "def process_text(text):\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #remove punctuation\n",
        "  text = re.sub(r'\\s\\s+', ' ', text) #replace double spaces with single spaces\n",
        "  text = text.lower() #set to lowercase\n",
        "  text = text.strip() #strip any additional whitespace\n",
        "  text = re.sub(r'\\d+', lambda x: num2words(int(x.group(0))), text) #replace numbers with string literals\n",
        "\n",
        "  return [word for word in nltk.word_tokenize(text) if word not in sw] #tokenize and remove stopwords\n",
        "\n",
        "def process_script(script_df):\n",
        "  processed_lines = [] #preprocess lines from script and add preprocessed lines back to dataframe\n",
        "  for i in range(len(script_df)):\n",
        "    processed_lines.append(process_text(script_df.loc[i, 'line']))\n",
        "  script_df['processed_lines'] = np.array(processed_lines)\n",
        "  script_df['line_count'] = script_df.groupby('character')['character'].transform('count')\n",
        "  return script_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4q7lG5_WEqg"
      },
      "source": [
        "\"\"\"\n",
        "Create WordCloud\n",
        "\"\"\"\n",
        "\n",
        "#CODE HERE FOUND AT https://amueller.github.io/word_cloud/auto_examples/a_new_hope.html\n",
        "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
        "                    **kwargs):\n",
        "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
        "\n",
        "def masked_wordcloud(img_path, script_df):\n",
        "  photo = np.array(Image.open(img_path)) #takes in an image path and dataframe, and generates a wordcloud for that image\n",
        "\n",
        "  wordcloud_text = ''\n",
        "  for i in range(len(script_df)):\n",
        "    wordcloud_text += ' '.join(script_df.loc[i, 'processed_lines'])\n",
        "\n",
        "  wc = WordCloud(background_color='black', mask=photo, stopwords=sw, margin=10, contour_color='grey', contour_width=2, repeat=True).generate(wordcloud_text)\n",
        "  # store default colored image\n",
        "  plt.figure(figsize=(20, 16))\n",
        "  plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3), interpolation=\"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2pwL0p7WIWq"
      },
      "source": [
        "\"\"\"\n",
        "Sentiment Analysis\n",
        "\"\"\"\n",
        "\n",
        "def sentiment_analysis(script_df):\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  for i in range(len(script_df)): #sentiment analysis for every line in the script\n",
        "    vader_scores = sid.polarity_scores(script_df.loc[i, 'line'])\n",
        "    for key in vader_scores:\n",
        "      script_df.loc[i, key] = vader_scores[key]\n",
        "  \n",
        "  return script_df\n",
        "\n",
        "def plot_sentiment_time_series(script_df):\n",
        "  df = script_df[script_df['line_count'] >= 15].reset_index()\n",
        "  fig = px.line(df, x='index', y='compound', color='character')\n",
        "\n",
        "  return fig\n",
        "\n",
        "def sentiment_bar(script_df):\n",
        "  bar_df = script_df[script_df['line_count'] >= 10] #filtering out characters with fewer than 10 lines\n",
        "  bar_df = bar_df.groupby('character').mean('compound').reset_index()\n",
        "  bar_df = bar_df[bar_df['compound'] != 0]\n",
        "  fig = px.bar(bar_df, x='character', y='compound', color='line_count', color_continuous_scale='Plasma', labels={'character': 'Character', 'compound': 'Average Sentiment', 'line_count': 'Line Count'})\n",
        "\n",
        "  return fig"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9uItD0zWLyJ"
      },
      "source": [
        "\"\"\"\n",
        "Embeddings\n",
        "\"\"\"\n",
        "\n",
        "def full_embeddings(script_df):\n",
        "  avg_embeddings = [] #average embeddings of all recognized words in every line\n",
        "  for i in range(len(script_df)):\n",
        "    line = script_df.loc[i, 'processed_lines']\n",
        "    embeddings = np.zeros(shape=(50,), dtype=np.float32)\n",
        "    recognized_words = 0\n",
        "    for word in line:\n",
        "      try:\n",
        "        embeddings += model[word]\n",
        "        recognized_words += 1\n",
        "      except KeyError:\n",
        "        pass\n",
        "    avg_embeddings.append(embeddings/recognized_words)\n",
        "  embeddings = pd.DataFrame(avg_embeddings)\n",
        "  embeddings = script_df[['character']].merge(embeddings, left_index=True, right_index=True)\n",
        "\n",
        "  return embeddings"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGyHXgMnWQ-G"
      },
      "source": [
        "\"\"\"\n",
        "Average Embeddings\n",
        "\"\"\"\n",
        "\n",
        "def average_embeddings(script_df):\n",
        "  embeddings = full_embeddings(script_df)\n",
        "  avg_embeddings = embeddings.groupby('character').mean().dropna() #getting average embeddings for each character for all their lines\n",
        "\n",
        "  return avg_embeddings"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFrnvk6OWXyU"
      },
      "source": [
        "\"\"\"\n",
        "TSNE\n",
        "\"\"\"\n",
        "\n",
        "def tsne_df(avg_embeddings, script_df):\n",
        "  explained_var = 0\n",
        "  i = 2\n",
        "  while explained_var < 0.975: #find the number of components with PCA which gives more than 97.5% explained variance, then conduct TSNE\n",
        "    pca = PCA(n_components=i, random_state=6)\n",
        "    pca.fit(avg_embeddings.to_numpy())\n",
        "    explained_var = pca.explained_variance_ratio_.sum()\n",
        "    i += 1\n",
        "  tsne = TSNE(n_components=2, perplexity=10.0, random_state=6)\n",
        "  pca = PCA(n_components=i).fit_transform(avg_embeddings.to_numpy(dtype=np.float32))\n",
        "\n",
        "  values = tsne.fit_transform(pca)\n",
        "  aggregated_embeddings = pd.DataFrame(values, columns=['x', 'y'], index=avg_embeddings.index)\n",
        "\n",
        "  return aggregated_embeddings.reset_index().merge(script_df.groupby('character').mean('compound').reset_index(), left_on='character', right_on='character')\n",
        "\n",
        "def plot_tsne(scatter_df):\n",
        "  fig = px.scatter(scatter_df, x='x', y='y', text='character', color='compound', range_color=(scatter_df['compound'].quantile(0.15), scatter_df['compound'].quantile(0.85)), labels={'character': 'Character', 'compound': 'Average Sentiment'}, title='TSNE Character Representations in 2D, Sized by Line Count')\n",
        "  fig.update_traces(marker = {'size': 10+scatter_df['line_count']}, textposition='top center')\n",
        "  \n",
        "  return fig"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU9nhU5bWcVY"
      },
      "source": [
        "\"\"\"\n",
        "Combining some functions\n",
        "\"\"\"\n",
        "\n",
        "def read_script_preprocess(script_name):\n",
        "  script_df = read_script(script_name) #read text\n",
        "  script_df = process_script(script_df) #process lines\n",
        "  script_df = sentiment_analysis(script_df) #conduct sentiment analysis\n",
        "  #goes from reading the script to being ready to get embeddings and/or make plots\n",
        "  return script_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4TEKGz0V8J_"
      },
      "source": [
        "# File Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFRv57nqvPqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2029b0-4c63-4f5d-a562-ca0fe64b3226"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('content')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIStBJ2h6rQw",
        "outputId": "35a974fc-bc2f-4628-a966-e6cfdcf1f6ea"
      },
      "source": [
        "os.listdir(SCRIPT_PATH)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SW_EpisodeVI.txt', 'SW_EpisodeV.txt', 'SW_EpisodeIV.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANCTO3CoCIrO"
      },
      "source": [
        "# getting sentiment/base statistics, embeddings, aggregated embeddings, and TSNE coordinates from scripts\n",
        "\n",
        "scripts = pd.DataFrame()\n",
        "embeds = pd.DataFrame()\n",
        "avg_embeds = pd.DataFrame()\n",
        "tsne = pd.DataFrame()\n",
        "\n",
        "for script in os.listdir(SCRIPT_PATH):\n",
        "  movie = script.replace('SW_', '')\n",
        "  movie = movie.replace('.txt', '')\n",
        "  df = read_script_preprocess(script)\n",
        "  embeddings = full_embeddings(df)\n",
        "  avg_embeddings = average_embeddings(df)\n",
        "\n",
        "  tsne_coords = tsne_df(avg_embeddings, df)\n",
        "  df.insert(1, 'film', movie)\n",
        "  embeddings.insert(1, 'film', movie)\n",
        "  avg_embeddings.insert(1, 'film', movie)\n",
        "  tsne_coords.insert(1, 'film', movie)\n",
        "\n",
        "  scripts=pd.concat([scripts, df])\n",
        "  embeds=pd.concat([embeds, embeddings])\n",
        "  avg_embeds=pd.concat([avg_embeds, avg_embeddings])\n",
        "  tsne=pd.concat([tsne, tsne_coords])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "iLkmV3o7EWBF",
        "outputId": "fa0b31b6-85cd-4198-e228-d06fac2c79fc"
      },
      "source": [
        "scripts"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>character</th>\n",
              "      <th>film</th>\n",
              "      <th>line</th>\n",
              "      <th>processed_lines</th>\n",
              "      <th>line_count</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OFFICER</td>\n",
              "      <td>EpisodeVI</td>\n",
              "      <td>Inform the commander that Lord Vader's shuttle...</td>\n",
              "      <td>[inform, commander, lord, vaders, shuttle, arr...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OPERATOR</td>\n",
              "      <td>EpisodeVI</td>\n",
              "      <td>Yes, sir.</td>\n",
              "      <td>[yes, sir]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.4019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JERJERROD</td>\n",
              "      <td>EpisodeVI</td>\n",
              "      <td>Lord Vader, this is an unexpected pleasure.  W...</td>\n",
              "      <td>[lord, vader, unexpected, pleasure, honored, p...</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.8176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VADER</td>\n",
              "      <td>EpisodeVI</td>\n",
              "      <td>You may dispense with the pleasantries, Comman...</td>\n",
              "      <td>[may, dispense, pleasantries, commander, put, ...</td>\n",
              "      <td>43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.859</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.3182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JERJERROD</td>\n",
              "      <td>EpisodeVI</td>\n",
              "      <td>I assure you, Lord Vader, my men are working a...</td>\n",
              "      <td>[assure, lord, vader, men, working, fast]</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.3400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>LUKE</td>\n",
              "      <td>EpisodeIV</td>\n",
              "      <td>Oh, no!</td>\n",
              "      <td>[oh]</td>\n",
              "      <td>254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>THREEPIO</td>\n",
              "      <td>EpisodeIV</td>\n",
              "      <td>Oh, my!  Artoo!  Can you hear me?  Say somethi...</td>\n",
              "      <td>[oh, artoo, hear, say, somethingyou, repair]</td>\n",
              "      <td>119</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>TECHNICIAN</td>\n",
              "      <td>EpisodeIV</td>\n",
              "      <td>We'll get to work on him right away.</td>\n",
              "      <td>[work, right, away]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>THREEPIO</td>\n",
              "      <td>EpisodeIV</td>\n",
              "      <td>You must repair him!  Sir, if any of my circui...</td>\n",
              "      <td>[must, repair, sir, circuits, gears, will, hel...</td>\n",
              "      <td>119</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.748</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.6588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>LUKE</td>\n",
              "      <td>EpisodeIV</td>\n",
              "      <td>He'll be all right.</td>\n",
              "      <td>[right]</td>\n",
              "      <td>254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2340 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      character       film  ...    pos compound\n",
              "0       OFFICER  EpisodeVI  ...  0.000   0.0000\n",
              "1      OPERATOR  EpisodeVI  ...  0.730   0.4019\n",
              "2     JERJERROD  EpisodeVI  ...  0.429   0.8176\n",
              "3         VADER  EpisodeVI  ...  0.141   0.3182\n",
              "4     JERJERROD  EpisodeVI  ...  0.156   0.3400\n",
              "..          ...        ...  ...    ...      ...\n",
              "889        LUKE  EpisodeIV  ...  0.000   0.0000\n",
              "890    THREEPIO  EpisodeIV  ...  0.000   0.0000\n",
              "891  TECHNICIAN  EpisodeIV  ...  0.000   0.0000\n",
              "892    THREEPIO  EpisodeIV  ...  0.252   0.6588\n",
              "893        LUKE  EpisodeIV  ...  0.000   0.0000\n",
              "\n",
              "[2340 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkUpB3zSFY2G"
      },
      "source": [
        "# exporting CSVs\n",
        "\n",
        "scripts.to_csv(os.path.join('/content/content/MyDrive/Star Wars Side Project', 'scripts.csv'), index=False)\n",
        "embeds.to_csv(os.path.join('/content/content/MyDrive/Star Wars Side Project', 'embeds.csv'), index=False)\n",
        "avg_embeds.to_csv(os.path.join('/content/content/MyDrive/Star Wars Side Project', 'avg_embeds.csv'), index=True)\n",
        "tsne.to_csv(os.path.join('/content/content/MyDrive/Star Wars Side Project', 'tsne.csv'), index=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsKSn8P2Xjsx"
      },
      "source": [
        "Line count information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyf6tUk1GHkf"
      },
      "source": [
        "from collections import Counter\n",
        "total_count = Counter()\n",
        "film_word_counts = dict()\n",
        "for film in scripts['film'].unique():\n",
        "  c = Counter()\n",
        "  for line in scripts[scripts['film'] == film]['processed_lines']:\n",
        "    c += Counter(line)\n",
        "    total_count += Counter(line)\n",
        "  film_word_counts[film] = pd.DataFrame.from_dict(c, orient='index').reset_index().rename(columns={'index': word, 0: 'count'}).sort_values(by='count', ascending=False)\n",
        "film_word_counts['all_films'] = pd.DataFrame.from_dict(total_count, orient='index').reset_index().rename(columns={'index': word, 0: 'count'}).sort_values(by='count', ascending=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D8K4zSMYpAp"
      },
      "source": [
        "film_word_counts['all_films'].to_csv(os.path.join(SCRIPT_PATH, '..', 'Outputs', 'word_counts.csv'), index=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge6TfmseYJCe"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "\n",
        "for film in scripts['film'].unique():\n",
        "  counts = film_word_counts[film]\n",
        "  counts.insert(1, 'film', film)\n",
        "  df = pd.concat([df, counts])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9_EQJtgY3u-"
      },
      "source": [
        "df.rename(columns={'no': 'word'}).sort_values(by='count', ascending=False).to_csv('word_counts_by_movie.csv', index=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsbIZ-WcA_zM"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}